# 🎯 О проекте Ollama Tray Chat

## Мотивация

Этот проект создан для предоставления простого и удобного способа взаимодействия с локальными LLM через Ollama на Linux-системах с KDE Plasma.

### Почему ещё один клиент?

- **Нативная интеграция с KDE** - системный трей, темы, горячие клавиши
- **Минимализм** - никаких лишних зависимостей, только PyQt6 и requests
- **Локальный AI** - полная конфиденциальность, работа без интернета
- **Простота** - установка одной командой, понятный интерфейс
- **Открытый код** - легко модифицировать под свои нужды

## Технический стек

- **Python 3.10+** - основной язык
- **PyQt6** - GUI фреймворк
- **Requests** - HTTP клиент для общения с Ollama API
- **Fish Shell** - скрипты установки/удаления
- **Ollama** - бэкенд для запуска LLM

## Архитектура

```
┌─────────────┐
│   PyQt6 UI  │  ← Пользовательский интерфейс
└──────┬──────┘
       │
┌──────▼──────┐
│ ChatWorker  │  ← Асинхронный обработчик запросов
│  (QThread)  │
└──────┬──────┘
       │
┌──────▼──────┐
│ Ollama API  │  ← REST API (streaming)
│ /api/chat   │
└──────┬──────┘
       │
┌──────▼──────┐
│   LLM Model │  ← Локальная языковая модель
└─────────────┘
```

## Особенности реализации

### Стриминг ответов
Используется `requests` с `stream=True` для получения ответов в реальном времени:
```python
with requests.post(url, json=payload, stream=True) as r:
    for line in r.iter_lines(decode_unicode=True):
        obj = json.loads(line)
        delta = obj.get("message", {}).get("content", "")
        self.chunk.emit(delta)  # Отправка в UI
```

### Многопоточность
`ChatWorker` наследует `QThread` для асинхронной обработки запросов без блокировки UI.

### Автосохранение
История сохраняется в формате JSONL (JSON Lines) для эффективного append и чтения.

### Системный трей
`QSystemTrayIcon` обеспечивает интеграцию с системным треем KDE.

## Файловая структура

```
ollama-tray-chat/
├── ollama_tray_chat.py       # Основное приложение (550+ строк)
├── icons/
│   └── ollama-chat.svg        # SVG иконка (масштабируемая)
├── resources/                  # Дополнительные ресурсы
├── ollama-tray-chat.desktop   # FreeDesktop entry point
├── install.fish               # Инсталлятор
├── uninstall.fish             # Деинсталлятор
├── run.fish                   # Быстрый запуск
├── requirements.txt           # Python зависимости
├── README.md                  # Основная документация
├── QUICKSTART.md              # Быстрый старт
├── SCREENSHOTS.md             # Описание интерфейса
├── LICENSE                    # MIT лицензия
└── .gitignore                 # Git исключения
```

## Пути в системе

После установки:
```
~/.config/ollama-tray-chat/
    └── config.json                    # Настройки (модель, промпт)

~/.local/share/ollama-tray-chat/
    └── history.jsonl                  # История чатов

~/.local/share/applications/
    └── ollama-tray-chat.desktop       # Ярлык приложения

~/.local/share/icons/hicolor/scalable/apps/
    └── ollama-chat.svg                # Иконка
```

## Протокол Ollama API

Приложение использует `/api/chat` endpoint:

**Запрос:**
```json
{
  "model": "phi3.5:3.8b-mini-instruct",
  "messages": [
    {"role": "system", "content": "Ты помощник..."},
    {"role": "user", "content": "Привет!"}
  ],
  "stream": true
}
```

**Ответ (streaming):**
```json
{"message": {"role": "assistant", "content": "П"}, "done": false}
{"message": {"role": "assistant", "content": "ри"}, "done": false}
{"message": {"role": "assistant", "content": "вет"}, "done": false}
...
{"done": true}
```

## Горячие клавиши

| Клавиша | Действие |
|---------|----------|
| `Enter` | Отправить сообщение |
| `Shift+Enter` | Новая строка в сообщении |
| `Ctrl+N` | Новый чат |
| `Ctrl+Q` | Выход из приложения |

## Производительность

- **Запуск**: < 1 секунда
- **Использование памяти**: ~50-80 MB (без учёта Ollama)
- **CPU**: минимальное (основная нагрузка на Ollama)
- **Сеть**: 0 (всё локально)

## Безопасность

✅ **Приватность**: Все данные остаются на локальной машине  
✅ **Открытый код**: Можно проверить весь исходный код  
✅ **Без телеметрии**: Никаких подключений к внешним серверам  
✅ **Локальные модели**: Полный контроль над AI моделью  

## Совместимость

Протестировано на:
- ✅ Arch Linux
- ✅ KDE Plasma 6.x
- ✅ Python 3.10, 3.11, 3.12, 3.13
- ✅ PyQt6 6.4+
- ✅ Ollama 0.1.x

Должно работать на:
- Manjaro, EndeavourOS (Arch-based)
- Другие дистрибутивы с KDE Plasma
- GNOME (с ограничениями в системном трее)

## Производительность моделей

| Модель | Размер | Память | Скорость | Качество |
|--------|--------|--------|----------|----------|
| phi3.5:3.8b | 2.3 GB | 4+ GB | ⚡⚡⚡ | ⭐⭐⭐ |
| llama2:7b | 3.8 GB | 8+ GB | ⚡⚡ | ⭐⭐⭐⭐ |
| mistral:7b | 4.1 GB | 8+ GB | ⚡⚡ | ⭐⭐⭐⭐⭐ |
| llama2:13b | 7.3 GB | 16+ GB | ⚡ | ⭐⭐⭐⭐⭐ |

## Известные ограничения

- Нет поддержки markdown в истории (пока что просто текст)
- Одна вкладка чата (планируется мультичат)
- Нет подсветки синтаксиса кода
- История не экспортируется (только JSONL файл)

## Планы развития

### v1.1
- [ ] Markdown рендеринг в истории
- [ ] Подсветка синтаксиса кода
- [ ] Тёмная тема

### v1.2
- [ ] Мультичат (вкладки)
- [ ] Экспорт истории (MD, HTML, PDF)
- [ ] Импорт/экспорт настроек

### v2.0
- [ ] Плагины
- [ ] Кастомные промпты
- [ ] RAG (документы как контекст)

## Вклад в проект

Приветствуются:
- 🐛 Баг-репорты
- 💡 Идеи новых функций
- 🔧 Pull requests
- 📝 Улучшения документации
- 🌍 Переводы

## Благодарности

- **Ollama Team** - за отличный инструмент локального AI
- **Riverbank Computing** - за PyQt6
- **KDE Community** - за лучший Linux desktop
- **Arch Linux** - за стабильность и rolling release

---

**Сделано с ♥ для сообщества Arch Linux**
